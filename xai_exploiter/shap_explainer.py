"""
SHAP Explainer Implementation
Author: Roshan Raundal
Date: September 2025
Purpose: Core SHAP explainability functionality for XAI Exploiter library
"""

import shap
import numpy as np
import pandas as pd
import warnings
from typing import Union, Optional, Any, Dict, List, Tuple
import joblib
from sklearn.base import BaseEstimator
import xgboost as xgb
import lightgbm as lgb

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')


class SHAPExplainer:
    """
    A comprehensive SHAP explainer class that provides model-agnostic explanations
    using different SHAP explainer types based on the model architecture.
    
    Features:
    - Automatic explainer selection (TreeExplainer, Explainer, KernelExplainer)
    - Support for tabular data (with future extensibility)
    - Unified interface for different model types
    - Comprehensive visualization support
    - Batch processing capabilities
    """
    
    def __init__(
        self, 
        model: Any, 
        background_data: Optional[Union[np.ndarray, pd.DataFrame]] = None,
        feature_names: Optional[List[str]] = None,
        explainer_type: str = "auto",
        max_background_size: int = 100
    ):
        """
        Initialize SHAP Explainer
        
        Args:
            model: Trained ML model (scikit-learn, XGBoost, LightGBM, etc.)
            background_data: Background dataset for explanation context
            feature_names: Names of input features
            explainer_type: Type of explainer ("auto", "tree", "kernel", "explainer")
            max_background_size: Maximum size of background dataset for efficiency
        """
        self.model = model
        self.feature_names = feature_names
        self.explainer_type = explainer_type
        self.max_background_size = max_background_size
        
        # Process background data
        self.background_data = self._process_background_data(background_data)
        
        # Initialize the appropriate SHAP explainer
        self.explainer = self._initialize_explainer()
        
        # Store explanation results
        self.shap_values = None
        self.expected_value = None
        
        print(f"‚úÖ SHAP Explainer initialized with {type(self.explainer).__name__}")
    
    def _process_background_data(
        self, 
        background_data: Optional[Union[np.ndarray, pd.DataFrame]]
    ) -> Optional[Union[np.ndarray, pd.DataFrame]]:
        """Process and sample background data if necessary"""
        if background_data is None:
            return None
            
        if isinstance(background_data, pd.DataFrame):
            if len(background_data) > self.max_background_size:
                return background_data.sample(n=self.max_background_size, random_state=42)
            return background_data
        elif isinstance(background_data, np.ndarray):
            if len(background_data) > self.max_background_size:
                indices = np.random.choice(
                    len(background_data), 
                    size=self.max_background_size, 
                    replace=False
                )
                return background_data[indices]
            return background_data
        else:
            raise ValueError("Background data must be pandas DataFrame or numpy array")
    
    def _is_tree_model(self) -> bool:
        """Check if the model is tree-based"""
        tree_model_names = [
            'XGBClassifier', 'XGBRegressor', 'XGBRanker',
            'LGBMClassifier', 'LGBMRegressor', 'LGBMRanker',
            'RandomForestClassifier', 'RandomForestRegressor',
            'ExtraTreesClassifier', 'ExtraTreesRegressor',
            'GradientBoostingClassifier', 'GradientBoostingRegressor',
            'DecisionTreeClassifier', 'DecisionTreeRegressor',
            'CatBoostClassifier', 'CatBoostRegressor'
        ]
        
        model_name = type(self.model).__name__
        return model_name in tree_model_names or hasattr(self.model, 'tree_')
    
    def _initialize_explainer(self) -> shap.Explainer:
        """Initialize the appropriate SHAP explainer based on model type"""
        try:
            if self.explainer_type == "tree" or (self.explainer_type == "auto" and self._is_tree_model()):
                print("üå≥ Using TreeExplainer for tree-based model")
                return shap.TreeExplainer(
                    self.model,
                    data=self.background_data,
                    feature_names=self.feature_names
                )

            
            elif self.explainer_type == "kernel":
                print("üîß Using KernelExplainer for model-agnostic approach")
                if self.background_data is None:
                    raise ValueError("Background data is required for KernelExplainer")
                
                # Create prediction function
                if hasattr(self.model, 'predict_proba'):
                    predict_fn = lambda x: self.model.predict_proba(x)[:, 1]
                else:
                    predict_fn = self.model.predict
                
                return shap.KernelExplainer(predict_fn, self.background_data)
            
            else:  # Default to general Explainer
                print("üéØ Using general Explainer")
                return shap.Explainer(
                    self.model, 
                    masker=self.background_data,
                    feature_names=self.feature_names
                )
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error initializing primary explainer: {e}")
            print("üîÑ Falling back to KernelExplainer")
            
            if self.background_data is None:
                raise ValueError("Background data is required for fallback KernelExplainer")
            
            predict_fn = self.model.predict
            return shap.KernelExplainer(predict_fn, self.background_data)
    
    def explain_instance(
        self, 
        instance: Union[np.ndarray, pd.DataFrame, pd.Series],
        **kwargs
    ) -> Dict[str, Any]:
        """
        Explain a single instance prediction
        
        Args:
            instance: Single instance to explain
            **kwargs: Additional arguments for SHAP explainer
            
        Returns:
            Dictionary containing explanation results
        """
        # Convert instance to proper format
        if isinstance(instance, pd.Series):
            instance = instance.to_frame().T
        elif isinstance(instance, np.ndarray):
            if instance.ndim == 1:
                instance = instance.reshape(1, -1)
        
        try:
            # Calculate SHAP values
            if isinstance(self.explainer, shap.TreeExplainer):
                shap_values = self.explainer.shap_values(instance, **kwargs)
                expected_value = self.explainer.expected_value
            else:
                explanation = self.explainer(instance, **kwargs)
                shap_values = explanation.values
                expected_value = explanation.base_values
            
            # Handle multi-class output
            if isinstance(shap_values, list):
                shap_values = shap_values[1] if len(shap_values) == 2 else shap_values[0]
            if isinstance(expected_value, (list, np.ndarray)) and len(expected_value) > 1:
                expected_value = expected_value[1] if len(expected_value) == 2 else expected_value[0]
            
            # Get prediction
            prediction = self.model.predict(instance)[0]
            
            return {
                'shap_values': shap_values[0] if shap_values.ndim > 1 else shap_values,
                'expected_value': float(expected_value) if isinstance(expected_value, (np.ndarray, list)) else expected_value,
                'prediction': prediction,
                'instance': instance,
                'feature_names': self.feature_names
            }
            
        except Exception as e:
            raise RuntimeError(f"Error explaining instance: {e}")
    
    def explain_global(
        self, 
        data: Union[np.ndarray, pd.DataFrame],
        max_samples: int = 1000,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Generate global explanations for dataset
        
        Args:
            data: Dataset to explain
            max_samples: Maximum number of samples to process
            **kwargs: Additional arguments for SHAP explainer
            
        Returns:
            Dictionary containing global explanation results
        """
        # Sample data if too large
        if len(data) > max_samples:
            if isinstance(data, pd.DataFrame):
                data = data.sample(n=max_samples, random_state=42)
            else:
                indices = np.random.choice(len(data), size=max_samples, replace=False)
                data = data[indices]
        
        try:
            # Calculate SHAP values
            if isinstance(self.explainer, shap.TreeExplainer):
                shap_values = self.explainer.shap_values(data, **kwargs)
                expected_value = self.explainer.expected_value
            else:
                explanation = self.explainer(data, **kwargs)
                shap_values = explanation.values
                expected_value = explanation.base_values
            
            # Handle multi-class output
            if isinstance(shap_values, list):
                shap_values = shap_values[1] if len(shap_values) == 2 else shap_values[0]
            
            # Store results for visualizations
            self.shap_values = shap_values
            self.expected_value = expected_value
            
            return {
                'shap_values': shap_values,
                'expected_value': expected_value,
                'data': data,
                'feature_names': self.feature_names,
                'feature_importance': np.abs(shap_values).mean(0)
            }
            
        except Exception as e:
            raise RuntimeError(f"Error generating global explanations: {e}")
    
    def get_feature_importance(
        self, 
        data: Union[np.ndarray, pd.DataFrame] = None,
        use_abs: bool = True
    ) -> pd.DataFrame:
        """
        Get feature importance based on SHAP values
        
        Args:
            data: Data to calculate importance from
            use_abs: Whether to use absolute values
            
        Returns:
            DataFrame with feature importance scores
        """
        if self.shap_values is None and data is not None:
            self.explain_global(data)
        
        if self.shap_values is None:
            raise ValueError("No SHAP values available. Run explain_global() first.")
        
        # Calculate importance
        if use_abs:
            importance_scores = np.abs(self.shap_values).mean(0)
        else:
            importance_scores = self.shap_values.mean(0)
        
        # Create DataFrame
        feature_names = self.feature_names or [f'Feature_{i}' for i in range(len(importance_scores))]
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': importance_scores
        }).sort_values('importance', ascending=False)
        
        return importance_df
    
    def save_model_explanation(self, filepath: str) -> None:
        """Save the explainer and explanations to file"""
        save_data = {
            'explainer': self.explainer,
            'shap_values': self.shap_values,
            'expected_value': self.expected_value,
            'feature_names': self.feature_names,
            'explainer_type': type(self.explainer).__name__
        }
        joblib.dump(save_data, filepath)
        print(f"üíæ Explainer saved to {filepath}")
    
    @classmethod
    def load_model_explanation(cls, filepath: str) -> 'SHAPExplainer':
        """Load explainer from file"""
        save_data = joblib.load(filepath)
        
        # Create a minimal instance
        explainer_obj = cls.__new__(cls)
        explainer_obj.explainer = save_data['explainer']
        explainer_obj.shap_values = save_data['shap_values']
        explainer_obj.expected_value = save_data['expected_value']
        explainer_obj.feature_names = save_data['feature_names']
        
        print(f"üìÅ Explainer loaded from {filepath}")
        return explainer_obj


# Utility functions for quick usage
def quick_explain(model, data, instance_idx: int = 0, **kwargs) -> Dict[str, Any]:
    """
    Quick explanation function for single instance
    
    Args:
        model: Trained ML model
        data: Training/background data
        instance_idx: Index of instance to explain
        **kwargs: Additional arguments
        
    Returns:
        Explanation dictionary
    """
    explainer = SHAPExplainer(model, background_data=data, **kwargs)
    instance = data.iloc[instance_idx:instance_idx+1] if isinstance(data, pd.DataFrame) else data[instance_idx:instance_idx+1]
    return explainer.explain_instance(instance)


def compare_models(models: List[Any], data, instance, model_names: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Compare SHAP explanations across multiple models
    
    Args:
        models: List of trained models
        data: Background data
        instance: Instance to explain
        model_names: Names for the models
        
    Returns:
        Comparison results
    """
    if model_names is None:
        model_names = [f"Model_{i+1}" for i in range(len(models))]
    
    results = {}
    for model, name in zip(models, model_names):
        try:
            explainer = SHAPExplainer(model, background_data=data)
            explanation = explainer.explain_instance(instance)
            results[name] = explanation
        except Exception as e:
            print(f"‚ö†Ô∏è Error explaining {name}: {e}")
            results[name] = None
    
    return results


if __name__ == "__main__":
    # Example usage and testing
    print("üß™ Testing SHAP Explainer...")
    
    # This would typically be run with actual model and data
    print("‚úÖ SHAP Explainer module loaded successfully!")
    print("üìñ Use: explainer = SHAPExplainer(your_model, your_data)")